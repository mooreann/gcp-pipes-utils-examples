{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test notebook for ShardVCFWf workflow\n",
    "quickly thrown together workflow using WDL\n",
    "takes a set of vcfs and shards by a set intervals (assumes bed format)\n",
    "I wouldn't normally run this from localhost mac notebook, but here have modified so hopefully it will"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    ##for this test purpose I'm using ldetect approx independent LD blocks form 1KG phase 1Europeans\n",
    "    ##need to liftover from hg19 to hg38\n",
    "    awk '$2 != \"start\"' /labshare/raph/datasets/ldetect/nygcresearch-ldetect-data-ac125e47bf7f/EUR/fourier_ls-all.bed \\\n",
    "    > /labshare/raph/datasets/ldetect/EUR.fourier_ls-all.hg19.bed\n",
    "\n",
    "    INBED=/labshare/raph/datasets/ldetect/EUR.fourier_ls-all.hg19.bed\n",
    "    CHAINFILE=/labseq/Genomes/Liftover_Chain_Files/hg19ToHg38.over.chain\n",
    "    OUTBED=/labshare/raph/datasets/ldetect/eur.fourier_ls-all.hg38.bed\n",
    "    UNMAPPEDBED=/labshare/raph/datasets/ldetect/eur.fourier_ls-all.hg38.unmapped.bed\n",
    "\n",
    "    /labseq/tools/ucsc/liftOver ${INBED} ${CHAINFILE} ${OUTBED} ${UNMAPPEDBED}\n",
    "\n",
    "    gsutil cp ${OUTBED} gs://nihnialng-ppmi-wgs/test/\n",
    "\n",
    "    gsutil ls -lh gs://nihnialng-ppmi-wgs/test/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pull down intervals file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir ~/data/ldetect\n",
    "gsutil cp gs://nihnialng-ppmi-wgs/test/eur.fourier_ls-all.hg38.bed ~/data/ldetect/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split intervals bed file by the chromosomes present in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bedpath = \"~/data/ldetect/eur.fourier_ls-all.hg38.bed\"\n",
    "outprefix = \"~/data/ldetect/eur.fourier_ls-all.hg38\"\n",
    "\n",
    "df = pd.read_table(bedpath,sep='\\t',header=None)\n",
    "df.columns = ['chr','start','stop']\n",
    "#save genome bed as individual chromosome beds\n",
    "chromosomes = df[\"chr\"].unique().tolist()\n",
    "for thisChr in chromosomes:\n",
    "    outfile = outprefix+\".\"+thisChr+\".bed\"\n",
    "    df[df[\"chr\"] == thisChr].to_csv(outfile,sep=\"\\t\",index=False,header=False)\n",
    "    outputFiles.append(outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh ~/data/ldetect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ~/data/ldetect/eur.fourier_ls-all.hg38.chr9.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#need to push per chr intervals to GCS\n",
    "#-m is threaded -q is quiet\n",
    "gsutil -mq cp ~/data/ldetect/eur.fourier_ls-all.hg38.chr*.bed gs://nihnialng-ppmi-wgs/test/intervals/\n",
    "    \n",
    "gsutil ls -lh gs://nihnialng-ppmi-wgs/test/intervals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so everything above this point was just setting up interval files for the jobs not actually running the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup local space for input json files\n",
    "!mkdir -p ~/data/shardvcfs/jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the WF input jsons\n",
    "import json\n",
    "\n",
    "json_template = \"/Users/gibbsr/code/scripts/ShardVCFWf.inputs.json\"\n",
    "\n",
    "interval_format = 'gs://nihnialng-ppmi-wgs/test/intervals/eur.fourier_ls-all.hg38.chr{thischr}.bed'\n",
    "vcf_format = 'gs://nihnialng-ppmi-wgs/hg38/genotypes/ppmi.july2018.chr{thischr}.vqsr.vcf.gz'\n",
    "vcf_index_format = 'gs://nihnialng-ppmi-wgs/hg38/genotypes/ppmi.july2018.chr{thischr}.vqsr.vcf.gz.tbi'\n",
    "basename = 'ppmi.july2018'\n",
    "\n",
    "chromosomes = list(map(str,range(1,23)))\n",
    "chromosomes.extend(['X','Y'])\n",
    "\n",
    "with open(json_template) as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    data[\"ShardVCFWf.basename_prefix\"] = basename\n",
    "    \n",
    "    for chromosome in chromosomes:\n",
    "        data[\"ShardVCFWf.intervals_list_bed\"] = interval_format.format(thischr=chromosome)\n",
    "        data[\"ShardVCFWf.input_vcf\"] = vcf_format.format(thischr=chromosome)\n",
    "        data[\"ShardVCFWf.input_vcf_index\"] = vcf_index_format.format(thischr=chromosome)\n",
    "        outfile = \"/Users/gibbsr/data/shardvcfs/jsons/chr\"+chromosome+\".json\"\n",
    "        with open(outfile,'w') as outfile:\n",
    "            json.dump(data,outfile,sort_keys=True,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#for PPMI the autosome vcf have vqsr in name but X and Y have raw, modify X and Y json\n",
    "#sed on linux good, sed on mac lame\n",
    "sed -i.bak s\"/vqsr/raw/\"g ~/data/shardvcfs/jsons/chrX.json\n",
    "sed -i.bak s\"/vqsr/raw/\"g ~/data/shardvcfs/jsons/chrY.json\n",
    "\n",
    "rm ~/data/shardvcfs/jsons/chrX.json.bak\n",
    "rm ~/data/shardvcfs/jsons/chrY.json.bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#pull down workflow file and generic options\n",
    "gsutil -mq cp gs://nihnialng-ppmi-wgs/test/ShardVCFWf.wdl data/shardvcfs/\n",
    "gsutil -mq cp gs://nihnialng-ppmi-wgs/test/generic.google-papi.options.json data/shardvcfs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#set some variables\n",
    "PROJECT_ID=pd-genome\n",
    "COHORT=ppmi\n",
    "COHORT_BUCKET=gs://nihnialng-ppmi-wgs\n",
    "PIPEYAML=gs://nihnialng-pd-wgs/tools/wdl_pipeline.yaml\n",
    "WFWDL=~/data/shardvcfs/ShardVCFWf.wdl\n",
    "WFOPTIONS=~/data/shardvcfs/generic.google-papi.options.json\n",
    "LCCOHORT=$(echo ${COHORT} | awk '{ print tolower($1) }')\n",
    "\n",
    "#execute the workflow (wf) on the cloud\n",
    "IN_CHROMOSOMES=\"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,X,Y\"\n",
    "CHRS=$(echo ${IN_CHROMOSOMES//,/ })\n",
    "for CHR in ${CHRS[@]}\n",
    "do\n",
    "LCCHR=$(echo ${CHR} | awk '{ print tolower($1) }')\n",
    "gcloud alpha genomics pipelines run \\\n",
    "--project ${PROJECT_ID} \\\n",
    "--pipeline-file ${PIPEYAML} \\\n",
    "--zones us-central1-f \\\n",
    "--memory 14 \\\n",
    "--logging ${COHORT_BUCKET}/test/logs/shardvcf \\\n",
    "--inputs-from-file WDL=${WFWDL} \\\n",
    "--inputs-from-file WORKFLOW_INPUTS=~/data/shardvcfs/jsons/chr${CHR}.json \\\n",
    "--inputs-from-file WORKFLOW_OPTIONS=${WFOPTIONS} \\\n",
    "--inputs WORKSPACE=${COHORT_BUCKET}/test/workspace \\\n",
    "--inputs OUTPUTS=${COHORT_BUCKET}/test/chr${CHR}shards/ \\\n",
    "--labels=pipe=shardvcf,cohort=${LCCOHORT},chrom=${LCCHR}\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#see if there are instances running the job\n",
    "PROJECT_ID=pd-genome\n",
    "COHORT=ppmi\n",
    "\n",
    "echo \"full job worker node count\"\n",
    "gcloud compute instances list --project ${PROJECT_ID} | grep RUNNING | wc -l\n",
    "echo \"job managers\"\n",
    "gcloud compute instances list --project ${PROJECT_ID} --filter \"labels.pipe=shardvcf labels.cohort=${COHORT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#grab the output operations ID from above and check if running\n",
    "OPID=EMHCoOzqLBiM99-2ze7__AkgpIDW36QGKg9wcm9kdWN0aW9uUXVldWU\n",
    "\n",
    "gcloud alpha genomics operations describe ${OPID} \\\n",
    "--format='yaml(done, error, metadata.events)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1250\n"
     ]
    }
   ],
   "source": [
    "#check that your files are there\n",
    "!gsutil ls -lh gs://nihnialng-ppmi-wgs/test/chr*shards/*.vcf.gz | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GGP WDL runner requires WF final output dir to be empty, since I iterated over chromosomes launching WF per chr \\\n",
    "#that means I output each to its own bucket path, so merge them into single bucket path\n",
    "!gsutil -mq mv gs://nihnialng-ppmi-wgs/test/chr*shards/*.vcf.gz gs://nihnialng-ppmi-wgs/test/shards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1626\n"
     ]
    }
   ],
   "source": [
    "#see if all the files are there\n",
    "!gsutil ls -lh gs://nihnialng-ppmi-wgs/test/shards/*.vcf.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that all the expected files are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://nihnialng-ppmi-wgs/test/shards/*.vcf.gz > ~/data/shardvcfs/found.files.txt\n",
    "sed -i.bak s\"/gs:\\/\\/nihnialng-ppmi-wgs\\/test\\/shards\\///\"g ~/data/shardvcfs/found.files.txt\n",
    "sed -i.bak s\"/ppmi.july2018_//\"g ~/data/shardvcfs/found.files.txt\n",
    "sed -i.bak s\"/.vcf.gz//\"g ~/data/shardvcfs/found.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr10_101189482_102620653\n",
      "chr10_10207433_12544798\n",
      "chr10_102620653_104935290\n",
      "chr10_104935290_106966928\n",
      "chr10_106966928_108557947\n",
      "chr10_108557947_110801735\n",
      "chr10_110801735_113568673\n",
      "chr10_113568673_114661647\n",
      "chr10_114661647_117764423\n",
      "chr10_117764423_118831841\n"
     ]
    }
   ],
   "source": [
    "!head ~/data/shardvcfs/found.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "Empty DataFrame\n",
      "Columns: [chr, start, stop]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "expectedFile = \"~/data/ldetect/eur.fourier_ls-all.hg38.bed\"\n",
    "foundFile = \"~/data/shardvcfs/found.files.txt\"\n",
    "outFile = \"~/data/shardvcfs/missing.hg38.bed\"\n",
    "\n",
    "expected = pd.read_table(expectedFile,sep='\\t',header=None)\n",
    "expected.columns = ['chr','start','stop']\n",
    "found = pd.read_table(foundFile,sep='\\t',header=None)\n",
    "found.columns = ['interval']\n",
    "\n",
    "expected['interval'] = expected[\"chr\"].astype(str)+\"_\"+expected[\"start\"].astype(str)+\"_\"+expected[\"stop\"].astype(str)\n",
    "\n",
    "missing = expected[~expected.interval.isin(found.interval)]\n",
    "missing.shape\n",
    "missing.head()\n",
    "\n",
    "missingOut = missing[['chr','start','stop']]\n",
    "print(missingOut.shape)\n",
    "print(missingOut.head())\n",
    "missingOut.to_csv(outFile,sep=\"\\t\",index=False,header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some still failed and are missing, hey it is complex broad stuff\n",
    "so could use new missing bed as input to shard wf or also just run with bash loop over a bcftools yaml\n",
    "both are not great clunky, keep repeating on missing til done, but that's how it is with a lot of these pipe lines\n",
    "this also gives examples of running yaml\n",
    "also looks like some of the cromwell instances were just doing nothing after a bit \n",
    "but don't have time to debug and tracing thru the cromwell is really unpleasant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#pull down the yaml\n",
    "#sometimes juypter bash won't execute gsutil commands correctly so you may have to run in terminal\n",
    "gsutil -m cp gs://nihnialng-ppmi-wgs/test/RegionSubsetVCF.yaml data/shardvcfs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi chr1\t109761309\t111199115 OPID=ppmi chr2\t180448012\t181401304 OPID=ppmi chr2\t235134906\t236540389 OPID=ppmi chr4\t124289707\t125559731 OPID=ppmi chr4\t187551827\t188772357 OPID="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running [operations/EJye34frLBiekeW7p5-6vXcgpIDW36QGKg9wcm9kdWN0aW9uUXVldWU].\n",
      "Running [operations/EKyk34frLBic1M-coIDSsEkgpIDW36QGKg9wcm9kdWN0aW9uUXVldWU].\n",
      "Running [operations/EP6q34frLBiGmcm31u_P_2IgpIDW36QGKg9wcm9kdWN0aW9uUXVldWU].\n",
      "Running [operations/EM6x34frLBiugsfv6-jVlB8gpIDW36QGKg9wcm9kdWN0aW9uUXVldWU].\n",
      "Running [operations/EPW334frLBjrztay1enoueMBIKSA1t-kBioPcHJvZHVjdGlvblF1ZXVl].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "COHORT=ppmi\n",
    "COHORTBUILD=${COHORT}.july2018\n",
    "PROJECT_ID=pd-genome\n",
    "PIPEYAML=~/data/shardvcfs/RegionSubsetVCF.yaml\n",
    "COHORT_BUCKET=gs://nihnialng-ppmi-wgs\n",
    "INTERVALFILE=~/data/shardvcfs/missing.hg38.bed\n",
    "\n",
    "LCCOHORT=$(echo ${COHORT} | awk '{ print tolower($1) }')\n",
    "\n",
    "while read -r INTERVAL\n",
    "do\n",
    "CHROMOSOME=$(echo ${INTERVAL} | awk '{print $1}')\n",
    "STARTBP=$(echo ${INTERVAL} | awk '{print $2}')\n",
    "STOPBP=$(echo ${INTERVAL} | awk '{print $3}')\n",
    "FRMTINTERVAL=$(echo \"${CHROMOSOME}_${STARTBP}_${STOPBP}\")\n",
    "LCINTERVAL=$(echo ${FRMTINTERVAL} | awk '{ print tolower($1) }')\n",
    "echo -n \"${COHORT} ${INTERVAL} OPID=\"\n",
    "\n",
    "gcloud alpha genomics pipelines run \\\n",
    "--project ${PROJECT_ID} \\\n",
    "--pipeline-file ${PIPEYAML} \\\n",
    "--preemptible \\\n",
    "--logging ${COHORT_BUCKET}/test/logs/regionsubsetvcf/ \\\n",
    "--inputs INPUTVCF=${COHORT_BUCKET}/hg38/genotypes/${COHORTBUILD}.${CHROMOSOME}.vqsr.vcf.gz \\\n",
    "--inputs INPUTVCFINDEX=${COHORT_BUCKET}/hg38/genotypes/${COHORTBUILD}.${CHROMOSOME}.vqsr.vcf.gz.tbi \\\n",
    "--inputs CHROMOSOME=${CHROMOSOME} \\\n",
    "--inputs STARTBP=${STARTBP} \\\n",
    "--inputs STOPBP=${STOPBP} \\\n",
    "--outputs OUTVCF=${COHORT_BUCKET}/test/shards/${COHORTBUILD}_${FRMTINTERVAL}.vcf.gz \\\n",
    "--outputs OUTVCFINDEX=${COHORT_BUCKET}/test/shards/${COHORTBUILD}_${FRMTINTERVAL}.vcf.gz.tbi \\\n",
    "--labels=pipe=regionsubsetvcf,cohort=${LCCOHORT},interval=${LCINTERVAL}\n",
    "done < ${INTERVALFILE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full job worker node count\n",
      "       0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#see if there are instances running the job\n",
    "PROJECT_ID=pd-genome\n",
    "COHORT=ppmi\n",
    "\n",
    "echo \"full job worker node count\"\n",
    "gcloud compute instances list --project ${PROJECT_ID} --filter \"labels.pipe=regionsubsetvcf labels.cohort=${COHORT}\" | grep RUNNING | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now go back up and recheck that all files are present and if not find missing, may want to run next pass of yaml without the \"--preemptible \\\" portion\n",
    "\n",
    "cell to go back up to\n",
    "    #see if all the files are there\n",
    "    !gsutil ls -lh gs://nihnialng-ppmi-wgs/test/shards/*.vcf.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "#if everything is done and looks right, clean up\n",
    "\n",
    "##be careful using wildcards, recursion and fat fingers on deletes\n",
    "#delete can be a little slow sometimes\n",
    "gsutil -mq rm -r gs://nihnialng-ppmi-wgs/test/logs/shardvcf\n",
    "gsutil -mq rm -r gs://nihnialng-ppmi-wgs/workspace/ShardVCFWf\n",
    "gsutil -mq rm -r gs://nihnialng-ppmi-wgs/test/chr*shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "!gsutil -mq rm -r gs://nihnialng-ppmi-wgs/test/logs/shardvcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -mq rm -r gs://nihnialng-ppmi-wgs/test/logs/regionsubsetvcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -mq rm -r gs://nihnialng-ppmi-wgs/test/workspace/ShardVCFWf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
